# encoding: utf-8
#
# Copyright (c) 2020-2021 Hopenly srl.
#
# This file is part of Ilyde.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import logging
import os
import shutil
from concurrent import futures
import grpc
import uuid
from cookiecutter.main import cookiecutter

from grpc_health.v1 import health, health_pb2_grpc
from grpc_interceptor.exceptions import InvalidArgument

from interceptors import ExceptionToStatusInterceptor
from protos import project_pb2_grpc
from models import documents
from serializers import status_serializer, search_project_request_serializer, \
    search_project_response_serializer, search_revision_request_serializer, search_revision_response_serializer
from serializers import project_serializer, id_serializer, revision_serializer
import datetime
import services
import utils


# setup logger
FORMAT = '%(asctime)s %(levelname)s %(message)s'
logging.basicConfig(level=logging.INFO, format=FORMAT)
logger = logging.getLogger(__name__)


class ProjectServicer(project_pb2_grpc.ProjectServicesServicer):

    def Retrieve(self, request, context):

        data = id_serializer.load(request)
        project = documents.Project.objects(deleted=False).get(id=data['id'])
        return project_serializer.dump(project)

    def Create(self, request, context):
        # validate request payload
        data = project_serializer.load(request)
        # parse create a bucket
        # slugify and create dataset name to create a bucket_name
        bucket_name = uuid.uuid4().hex
        # minio create bucket
        services.create_minio_bucket(bucket_name)
        # creation of a configuration file
        config = utils.create_config(data['name'], data['description'])
        # creation of a cookiecutter template
        workdir = os.path.join(os.path.dirname(__file__), 'data', bucket_name)
        templatedir = os.path.join(os.path.dirname(__file__), 'templates', data['template'].lower())
        os.mkdir(workdir)

        cookiecutter(templatedir, output_dir=workdir, no_input=True,
                     extra_context=config)

        # copy files back to minio bucket
        file_tree = services.sync_dir_to_minio_bucket(bucket_name,
                                                      os.path.join(workdir, config.get('repo_name')))
        # save to db
        members = data["members"]
        members.append(data["owner"])
        data['members'] = list(set(members))

        project = documents.Project(name=data['name'],
                                    description=data['description'],
                                    visibility=data['visibility'],
                                    template=data['template'],
                                    owner=data['owner'],
                                    members=data['members'],
                                    state=data["state"],
                                    repo_bucket=bucket_name).save()
        documents.Revision(commit="Generated by Ilyde", project=project.id, author=data['owner'],
                           file_tree=file_tree).save()
        # clean all
        shutil.rmtree(workdir)
        return project_serializer.dump(project)

    def Update(self, request, context):
        # validate request payload
        data = project_serializer.load(request)
        # validate id
        if not data.get('id'):
            raise InvalidArgument("Project's id not provided.")

        # retrieve project
        project = documents.Project.objects(deleted=False).get(id=data['id'])
        # update members, state and description.
        if data['description']:
            project.description = data['description']

        if data['members']:
            project.members = data['members']

        if data['state']:
            project.state = data['state']

        project.last_update = datetime.datetime.now()
        project.save()

        return project_serializer.dump(project)

    def Delete(self, request, context):
        # validate request payload
        data = id_serializer.load(request)
        project = documents.Project.objects(deleted=False).get(id=data['id'])
        project.deleted = True
        project.save()

        return status_serializer.dump({"status": 200, "message": "Successfully delete dataset."})

    def Search(self, request, context):
        data = search_project_request_serializer.load(request)
        mappings = {
            "id": "_id",
            "name": "name",
            "visibility": "visibility",
            "template": "template",
            "state": "state",
            "member": "members"
        }
        ids = ["_id"]

        query = utils.construct_mongo_query(data["query"], mappings, ids)
        if query:
            projects = documents.Project.objects(__raw__=query).filter(deleted=False)
        else:
            projects = documents.Project.objects(deleted=False)

        paginated = project_serializer.paginate(projects, page=data["page"],
                                                limit=data["limit"])
        payload = {
            "total": len(projects),
            "page": paginated[0],
            "limit": paginated[1],
            "data": paginated[2]
        }
        return search_project_response_serializer.dump(payload)

    def RetrieveRevision(self, request, context):
        # validate request payload
        data = id_serializer.load(request)
        revision = documents.Revision.objects(
                project__in=documents.Project.objects(deleted=False)).get(id=data['id'])
        return revision_serializer.dump(revision)

    def CreateRevision(self, request, context):
        # validate request payload
        data = revision_serializer.load(request)
        # retrieve dataset
        project = documents.Project.objects(deleted=False).get(id=data['project'])
        # retrieve repo bucket
        file_tree = services.list_minio_bucket_objects(project.repo_bucket)
        # save new revision
        revision = documents.Revision(commit=data['commit'], project=project.id,
                                      author=data['author'], file_tree=file_tree).save()
        project.last_update = datetime.datetime.now()
        project.save()

        return revision_serializer.dump(revision)

    def SearchRevision(self, request, context):
        data = search_revision_request_serializer.load(request)

        mappings = {
            "id": "_id",
            "project": "project",
            "author": "author",
        }
        ids = ["_id", "project"]

        query = utils.construct_mongo_query(data["query"], mappings, ids)
        if query:
            revisions = documents.Revision.objects(__raw__=query).filter(
                project__in=documents.Project.objects(deleted=False))
        else:
            revisions = documents.Revision.objects(
                project__in=documents.Project.objects(deleted=False))

        paginated = project_serializer.paginate(revisions, page=data["page"],
                                                limit=data["limit"])
        payload = {
            "total": len(revisions),
            "page": paginated[0],
            "limit": paginated[1],
            "data": paginated[2]
        }
        return search_revision_response_serializer.dump(payload)


def create_server(server_address):
    interceptors = [ExceptionToStatusInterceptor()]
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10), interceptors=interceptors)
    project_pb2_grpc.add_ProjectServicesServicer_to_server(
        ProjectServicer(), server
    )
    # Create a health check servicer. We use the non-blocking implementation
    # to avoid thread starvation.
    health_servicer = health.HealthServicer(
        experimental_non_blocking=True,
        experimental_thread_pool=futures.ThreadPoolExecutor(max_workers=1))
    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)
    port = server.add_insecure_port(server_address)
    return server, port


def serve():
    server, port = create_server('[::]:50051')
    server.start()
    logger.info("server is serving on port {} ............".format(port))
    server.wait_for_termination()
    logger.info("server is stopped............")


if __name__ == '__main__':
    logger.info("server is starting............")
    serve()
